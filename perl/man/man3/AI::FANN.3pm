.\" Automatically generated by Pod::Man 2.22 (Pod::Simple 3.07)
.\"
.\" Standard preamble:
.\" ========================================================================
.de Sp \" Vertical space (when we can't use .PP)
.if t .sp .5v
.if n .sp
..
.de Vb \" Begin verbatim text
.ft CW
.nf
.ne \\$1
..
.de Ve \" End verbatim text
.ft R
.fi
..
.\" Set up some character translations and predefined strings.  \*(-- will
.\" give an unbreakable dash, \*(PI will give pi, \*(L" will give a left
.\" double quote, and \*(R" will give a right double quote.  \*(C+ will
.\" give a nicer C++.  Capital omega is used to do unbreakable dashes and
.\" therefore won't be available.  \*(C` and \*(C' expand to `' in nroff,
.\" nothing in troff, for use with C<>.
.tr \(*W-
.ds C+ C\v'-.1v'\h'-1p'\s-2+\h'-1p'+\s0\v'.1v'\h'-1p'
.ie n \{\
.    ds -- \(*W-
.    ds PI pi
.    if (\n(.H=4u)&(1m=24u) .ds -- \(*W\h'-12u'\(*W\h'-12u'-\" diablo 10 pitch
.    if (\n(.H=4u)&(1m=20u) .ds -- \(*W\h'-12u'\(*W\h'-8u'-\"  diablo 12 pitch
.    ds L" ""
.    ds R" ""
.    ds C` ""
.    ds C' ""
'br\}
.el\{\
.    ds -- \|\(em\|
.    ds PI \(*p
.    ds L" ``
.    ds R" ''
'br\}
.\"
.\" Escape single quotes in literal strings from groff's Unicode transform.
.ie \n(.g .ds Aq \(aq
.el       .ds Aq '
.\"
.\" If the F register is turned on, we'll generate index entries on stderr for
.\" titles (.TH), headers (.SH), subsections (.SS), items (.Ip), and index
.\" entries marked with X<> in POD.  Of course, you'll have to process the
.\" output yourself in some meaningful fashion.
.ie \nF \{\
.    de IX
.    tm Index:\\$1\t\\n%\t"\\$2"
..
.    nr % 0
.    rr F
.\}
.el \{\
.    de IX
..
.\}
.\"
.\" Accent mark definitions (@(#)ms.acc 1.5 88/02/08 SMI; from UCB 4.2).
.\" Fear.  Run.  Save yourself.  No user-serviceable parts.
.    \" fudge factors for nroff and troff
.if n \{\
.    ds #H 0
.    ds #V .8m
.    ds #F .3m
.    ds #[ \f1
.    ds #] \fP
.\}
.if t \{\
.    ds #H ((1u-(\\\\n(.fu%2u))*.13m)
.    ds #V .6m
.    ds #F 0
.    ds #[ \&
.    ds #] \&
.\}
.    \" simple accents for nroff and troff
.if n \{\
.    ds ' \&
.    ds ` \&
.    ds ^ \&
.    ds , \&
.    ds ~ ~
.    ds /
.\}
.if t \{\
.    ds ' \\k:\h'-(\\n(.wu*8/10-\*(#H)'\'\h"|\\n:u"
.    ds ` \\k:\h'-(\\n(.wu*8/10-\*(#H)'\`\h'|\\n:u'
.    ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'^\h'|\\n:u'
.    ds , \\k:\h'-(\\n(.wu*8/10)',\h'|\\n:u'
.    ds ~ \\k:\h'-(\\n(.wu-\*(#H-.1m)'~\h'|\\n:u'
.    ds / \\k:\h'-(\\n(.wu*8/10-\*(#H)'\z\(sl\h'|\\n:u'
.\}
.    \" troff and (daisy-wheel) nroff accents
.ds : \\k:\h'-(\\n(.wu*8/10-\*(#H+.1m+\*(#F)'\v'-\*(#V'\z.\h'.2m+\*(#F'.\h'|\\n:u'\v'\*(#V'
.ds 8 \h'\*(#H'\(*b\h'-\*(#H'
.ds o \\k:\h'-(\\n(.wu+\w'\(de'u-\*(#H)/2u'\v'-.3n'\*(#[\z\(de\v'.3n'\h'|\\n:u'\*(#]
.ds d- \h'\*(#H'\(pd\h'-\w'~'u'\v'-.25m'\f2\(hy\fP\v'.25m'\h'-\*(#H'
.ds D- D\\k:\h'-\w'D'u'\v'-.11m'\z\(hy\v'.11m'\h'|\\n:u'
.ds th \*(#[\v'.3m'\s+1I\s-1\v'-.3m'\h'-(\w'I'u*2/3)'\s-1o\s+1\*(#]
.ds Th \*(#[\s+2I\s-2\h'-\w'I'u*3/5'\v'-.3m'o\v'.3m'\*(#]
.ds ae a\h'-(\w'a'u*4/10)'e
.ds Ae A\h'-(\w'A'u*4/10)'E
.    \" corrections for vroff
.if v .ds ~ \\k:\h'-(\\n(.wu*9/10-\*(#H)'\s-2\u~\d\s+2\h'|\\n:u'
.if v .ds ^ \\k:\h'-(\\n(.wu*10/11-\*(#H)'\v'-.4m'^\v'.4m'\h'|\\n:u'
.    \" for low resolution devices (crt and lpr)
.if \n(.H>23 .if \n(.V>19 \
\{\
.    ds : e
.    ds 8 ss
.    ds o a
.    ds d- d\h'-1'\(ga
.    ds D- D\h'-1'\(hy
.    ds th \o'bp'
.    ds Th \o'LP'
.    ds ae ae
.    ds Ae AE
.\}
.rm #[ #] #H #V #F C
.\" ========================================================================
.\"
.IX Title "AI::FANN 3pm"
.TH AI::FANN 3pm "2009-03-10" "perl v5.10.1" "User Contributed Perl Documentation"
.\" For nroff, turn off justification.  Always turn off hyphenation; it makes
.\" way too many mistakes in technical documents.
.if n .ad l
.nh
.SH "NAME"
AI::FANN \- Perl wrapper for the Fast Artificial Neural Network library
.SH "SYNOPSIS"
.IX Header "SYNOPSIS"
Train...
.PP
.Vb 1
\&  use AI::FANN qw(:all);
\&
\&  # create an ANN with 2 inputs, a hidden layer with 3 neurons and an
\&  # output layer with 1 neuron:
\&  my $ann = AI::FANN\->new_standard(2, 3, 1);
\&
\&  $ann\->hidden_activation_function(FANN_SIGMOID_SYMMETRIC);
\&  $ann\->output_activation_function(FANN_SIGMOID_SYMMETRIC);
\&
\&  # create the training data for a XOR operator:
\&  my $xor_train = AI::FANN::TrainData\->new( [\-1, \-1], [\-1],
\&                                            [\-1, 1], [1],
\&                                            [1, \-1], [1],
\&                                            [1, 1], [\-1] );
\&
\&  $ann\->train_on_data($xor_train, 500000, 1000, 0.001);
\&
\&  $ann\->save("xor.ann");
.Ve
.PP
Run...
.PP
.Vb 1
\&  use AI::FANN;
\&
\&  my $ann = AI::FANN\->new_from_file("xor.ann");
\&
\&  for my $a (\-1, 1) {
\&    for my $b (\-1, 1) {
\&      my $out = $ann\->run([$a, $b]);
\&      printf "xor(%f, %f) = %f\en", $a, $b, $out\->[0];
\&    }
\&  }
.Ve
.SH "DESCRIPTION"
.IX Header "DESCRIPTION"
.Vb 2
\&  WARNING:  THIS IS A VERY EARLY RELEASE,
\&            MAY CONTAIN CRITICAL BUGS!!!
.Ve
.PP
\&\s-1AI::FANN\s0 is a Perl wrapper for the Fast Artificial Neural Network
(\s-1FANN\s0) Library available from <http://fann.sourceforge.net>:
.PP
.Vb 11
\&  Fast Artificial Neural Network Library is a free open source neural
\&  network library, which implements multilayer artificial neural
\&  networks in C with support for both fully connected and sparsely
\&  connected networks. Cross\-platform execution in both fixed and
\&  floating point are supported. It includes a framework for easy
\&  handling of training data sets. It is easy to use, versatile, well
\&  documented, and fast. PHP, C++, .NET, Python, Delphi, Octave, Ruby,
\&  Pure Data and Mathematica bindings are available. A reference manual
\&  accompanies the library with examples and recommendations on how to
\&  use the library. A graphical user interface is also available for
\&  the library.
.Ve
.PP
\&\s-1AI::FANN\s0 object oriented interface provides an almost direct map to
the C library \s-1API\s0. Some differences have been introduced to make it
more perlish:
.IP "\(bu" 4
Two classes are used: \f(CW\*(C`AI::FANN\*(C'\fR that wraps the C \f(CW\*(C`struct fann\*(C'\fR type
and \f(CW\*(C`AI::FANN::TrainData\*(C'\fR that wraps \f(CW\*(C`struct fann_train_data\*(C'\fR.
.IP "\(bu" 4
Prefixes and common parts on the C function names referring to those
structures have been removed. For instance C
\&\f(CW\*(C`fann_train_data_shuffle\*(C'\fR becomes \f(CW\*(C`AI::FANN::TrainData::shuffle\*(C'\fR that
will be usually called as...
.Sp
.Vb 1
\&  $train_data\->shuffle;
.Ve
.IP "\(bu" 4
Pairs of C get/set functions are wrapped in Perl with dual accessor
methods named as the attribute (and without any \f(CW\*(C`set_\*(C'\fR/\f(CW\*(C`get_\*(C'\fR
prefix). For instance:
.Sp
.Vb 1
\&  $ann\->bit_fail_limit($limit); # sets the bit_fail_limit
\&
\&  $bfl = $ann\->bit_fail_limit;  # gets the bit_fail_limit
.Ve
.Sp
Pairs of get/set functions requiring additional indexing arguments are
also wrapped inside dual accessors:
.Sp
.Vb 2
\&  # sets:
\&  $ann\->neuron_activation_function($layer_ix, $neuron_ix, $actfunc);
\&
\&  # gets:
\&  $af = $ann\->neuron_activation_function($layer_ix, $neuron_ix);
.Ve
.Sp
Important: note that on the Perl version, the optional value argument
is moved to the last position (on the C version of the \f(CW\*(C`set_\*(C'\fR method
it is usually the second argument).
.IP "\(bu" 4
Some functions have been renamed to make the naming more consistent
and to follow Perl conventions:
.Sp
.Vb 10
\&  C                                      Perl
\&  \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-
\&  fann_create_from_file               => new_from_file
\&  fann_create_standard                => new_standard
\&  fann_get_num_input                  => num_inputs
\&  fann_get_activation_function        => neuron_activation_function
\&  fann_set_activation_function        => ^^^
\&  fann_set_activation_function_layer  => layer_activation_function
\&  fann_set_activation_function_hidden => hidden_activation_function
\&  fann_set_activation_function_output => output_activation_function
.Ve
.IP "\(bu" 4
Boolean methods return true on success and undef on failure.
.IP "\(bu" 4
Any error reported from the C side is automaticaly converter to a Perl
exception. No manual error checking is required after calling \s-1FANN\s0
functions.
.IP "\(bu" 4
Memory management is automatic, no need to call destroy methods.
.IP "\(bu" 4
Doubles are used for computations (using floats or fixed
point types is not supported).
.SH "CONSTANTS"
.IX Header "CONSTANTS"
All the constants defined in the C documentation are exported from the module:
.PP
.Vb 2
\&  # import all...
\&  use AI::FANN \*(Aq:all\*(Aq;
\&
\&  # or individual constants...
\&  use AI::FANN qw(FANN_TRAIN_INCREMENTAL FANN_GAUSSIAN);
.Ve
.PP
The values returned from this constant subs yield the integer value on
numerical context and the constant name when used as strings.
.PP
The constants available are:
.PP
.Vb 5
\&  # enum fann_train_enum:
\&  FANN_TRAIN_INCREMENTAL
\&  FANN_TRAIN_BATCH
\&  FANN_TRAIN_RPROP
\&  FANN_TRAIN_QUICKPROP
\&
\&  # enum fann_activationfunc_enum:
\&  FANN_LINEAR
\&  FANN_THRESHOLD
\&  FANN_THRESHOLD_SYMMETRIC
\&  FANN_SIGMOID
\&  FANN_SIGMOID_STEPWISE
\&  FANN_SIGMOID_SYMMETRIC
\&  FANN_SIGMOID_SYMMETRIC_STEPWISE
\&  FANN_GAUSSIAN
\&  FANN_GAUSSIAN_SYMMETRIC
\&  FANN_GAUSSIAN_STEPWISE
\&  FANN_ELLIOT
\&  FANN_ELLIOT_SYMMETRIC
\&  FANN_LINEAR_PIECE
\&  FANN_LINEAR_PIECE_SYMMETRIC
\&  FANN_SIN_SYMMETRIC
\&  FANN_COS_SYMMETRIC
\&  FANN_SIN
\&  FANN_COS
\&
\&  # enum fann_errorfunc_enum:
\&  FANN_ERRORFUNC_LINEAR
\&  FANN_ERRORFUNC_TANH
\&
\&  # enum fann_stopfunc_enum:
\&  FANN_STOPFUNC_MSE
\&  FANN_STOPFUNC_BIT
.Ve
.SH "CLASSES"
.IX Header "CLASSES"
The classes defined by this package are:
.SS "\s-1AI::FANN\s0"
.IX Subsection "AI::FANN"
Wraps C \f(CW\*(C`struct fann\*(C'\fR types and provides the following methods
(consult the C documentation for a full description of their usage):
.IP "\s-1AI::FANN\-\s0>new_standard(@layer_sizes)" 4
.IX Item "AI::FANN->new_standard(@layer_sizes)"
\&\-
.ie n .IP "\s-1AI::FANN\-\s0>new_sparse($connection_rate, @layer_sizes)" 4
.el .IP "\s-1AI::FANN\-\s0>new_sparse($connection_rate, \f(CW@layer_sizes\fR)" 4
.IX Item "AI::FANN->new_sparse($connection_rate, @layer_sizes)"
\&\-
.IP "\s-1AI::FANN\-\s0>new_shortcut(@layer_sizes)" 4
.IX Item "AI::FANN->new_shortcut(@layer_sizes)"
\&\-
.IP "\s-1AI::FANN\-\s0>new_from_file($filename)" 4
.IX Item "AI::FANN->new_from_file($filename)"
\&\-
.ie n .IP "$ann\->save($filename)" 4
.el .IP "\f(CW$ann\fR\->save($filename)" 4
.IX Item "$ann->save($filename)"
\&\-
.ie n .IP "$ann\->run($input)" 4
.el .IP "\f(CW$ann\fR\->run($input)" 4
.IX Item "$ann->run($input)"
\&\f(CW\*(C`input\*(C'\fR is an array with the input values.
.Sp
returns an array with the values on the output layer.
.Sp
.Vb 2
\&  $out = $ann\->run([1, 0.6]);
\&  print "@$out\en";
.Ve
.ie n .IP "$ann\->randomize_weights($min_weight, $max_weight)" 4
.el .IP "\f(CW$ann\fR\->randomize_weights($min_weight, \f(CW$max_weight\fR)" 4
.IX Item "$ann->randomize_weights($min_weight, $max_weight)"
.PD 0
.ie n .IP "$ann\->train($input, $desired_output)" 4
.el .IP "\f(CW$ann\fR\->train($input, \f(CW$desired_output\fR)" 4
.IX Item "$ann->train($input, $desired_output)"
.PD
\&\f(CW$input\fR and \f(CW$desired_output\fR are arrays.
.ie n .IP "$ann\->test($input, $desired_output)" 4
.el .IP "\f(CW$ann\fR\->test($input, \f(CW$desired_output\fR)" 4
.IX Item "$ann->test($input, $desired_output)"
\&\f(CW$input\fR and \f(CW$desired_output\fR are arrays.
.Sp
It returns an array with the values of the output layer.
.ie n .IP "$ann\->reset_MSE" 4
.el .IP "\f(CW$ann\fR\->reset_MSE" 4
.IX Item "$ann->reset_MSE"
\&\-
.ie n .IP "$ann\->train_on_file($filename, $max_epochs, $epochs_between_reports, $desired_error)" 4
.el .IP "\f(CW$ann\fR\->train_on_file($filename, \f(CW$max_epochs\fR, \f(CW$epochs_between_reports\fR, \f(CW$desired_error\fR)" 4
.IX Item "$ann->train_on_file($filename, $max_epochs, $epochs_between_reports, $desired_error)"
\&\-
.ie n .IP "$ann\->train_on_data($train_data, $max_epochs, $epochs_between_reports, $desired_error)" 4
.el .IP "\f(CW$ann\fR\->train_on_data($train_data, \f(CW$max_epochs\fR, \f(CW$epochs_between_reports\fR, \f(CW$desired_error\fR)" 4
.IX Item "$ann->train_on_data($train_data, $max_epochs, $epochs_between_reports, $desired_error)"
\&\f(CW$train_data\fR is a AI::FANN::TrainData object.
.ie n .IP "$ann\->cascadetrain_on_file($filename, $max_neurons, $neurons_between_reports, $desired_error)" 4
.el .IP "\f(CW$ann\fR\->cascadetrain_on_file($filename, \f(CW$max_neurons\fR, \f(CW$neurons_between_reports\fR, \f(CW$desired_error\fR)" 4
.IX Item "$ann->cascadetrain_on_file($filename, $max_neurons, $neurons_between_reports, $desired_error)"
\&\-
.ie n .IP "$ann\->cascadetrain_on_data($train_data, $max_neurons, $neurons_between_reports, $desired_error)" 4
.el .IP "\f(CW$ann\fR\->cascadetrain_on_data($train_data, \f(CW$max_neurons\fR, \f(CW$neurons_between_reports\fR, \f(CW$desired_error\fR)" 4
.IX Item "$ann->cascadetrain_on_data($train_data, $max_neurons, $neurons_between_reports, $desired_error)"
\&\f(CW$train_data\fR is a AI::FANN::TrainData object.
.ie n .IP "$ann\->train_epoch($train_data)" 4
.el .IP "\f(CW$ann\fR\->train_epoch($train_data)" 4
.IX Item "$ann->train_epoch($train_data)"
\&\f(CW$train_data\fR is a AI::FANN::TrainData object.
.ie n .IP "$ann\->print_connections" 4
.el .IP "\f(CW$ann\fR\->print_connections" 4
.IX Item "$ann->print_connections"
\&\-
.ie n .IP "$ann\->print_parameters" 4
.el .IP "\f(CW$ann\fR\->print_parameters" 4
.IX Item "$ann->print_parameters"
\&\-
.ie n .IP "$ann\->\fIcascade_activation_functions()\fR" 4
.el .IP "\f(CW$ann\fR\->\fIcascade_activation_functions()\fR" 4
.IX Item "$ann->cascade_activation_functions()"
returns a list of the activation functions used for cascade training.
.ie n .IP "$ann\->cascade_activation_functions(@activation_functions)" 4
.el .IP "\f(CW$ann\fR\->cascade_activation_functions(@activation_functions)" 4
.IX Item "$ann->cascade_activation_functions(@activation_functions)"
sets the list of activation function to use for cascade training.
.ie n .IP "$ann\->\fIcascade_activation_steepnesses()\fR" 4
.el .IP "\f(CW$ann\fR\->\fIcascade_activation_steepnesses()\fR" 4
.IX Item "$ann->cascade_activation_steepnesses()"
returns a list of the activation steepnesses used for cascade training.
.ie n .IP "$ann\->cascade_activation_steepnesses(@activation_steepnesses)" 4
.el .IP "\f(CW$ann\fR\->cascade_activation_steepnesses(@activation_steepnesses)" 4
.IX Item "$ann->cascade_activation_steepnesses(@activation_steepnesses)"
sets the list of activation steepnesses to use for cascade training.
.ie n .IP "$ann\->training_algorithm" 4
.el .IP "\f(CW$ann\fR\->training_algorithm" 4
.IX Item "$ann->training_algorithm"
.PD 0
.ie n .IP "$ann\->training_algorithm($training_algorithm)" 4
.el .IP "\f(CW$ann\fR\->training_algorithm($training_algorithm)" 4
.IX Item "$ann->training_algorithm($training_algorithm)"
.PD
\&\-
.ie n .IP "$ann\->train_error_function" 4
.el .IP "\f(CW$ann\fR\->train_error_function" 4
.IX Item "$ann->train_error_function"
.PD 0
.ie n .IP "$ann\->train_error_function($error_function)" 4
.el .IP "\f(CW$ann\fR\->train_error_function($error_function)" 4
.IX Item "$ann->train_error_function($error_function)"
.PD
\&\-
.ie n .IP "$ann\->train_stop_function" 4
.el .IP "\f(CW$ann\fR\->train_stop_function" 4
.IX Item "$ann->train_stop_function"
.PD 0
.ie n .IP "$ann\->train_stop_function($stop_function)" 4
.el .IP "\f(CW$ann\fR\->train_stop_function($stop_function)" 4
.IX Item "$ann->train_stop_function($stop_function)"
.PD
\&\-
.ie n .IP "$ann\->learning_rate" 4
.el .IP "\f(CW$ann\fR\->learning_rate" 4
.IX Item "$ann->learning_rate"
.PD 0
.ie n .IP "$ann\->learning_rate($rate)" 4
.el .IP "\f(CW$ann\fR\->learning_rate($rate)" 4
.IX Item "$ann->learning_rate($rate)"
.PD
\&\-
.ie n .IP "$ann\->learning_momentum" 4
.el .IP "\f(CW$ann\fR\->learning_momentum" 4
.IX Item "$ann->learning_momentum"
.PD 0
.ie n .IP "$ann\->learning_momentum($momentun)" 4
.el .IP "\f(CW$ann\fR\->learning_momentum($momentun)" 4
.IX Item "$ann->learning_momentum($momentun)"
.PD
\&\-
.ie n .IP "$ann\->bit_fail_limit" 4
.el .IP "\f(CW$ann\fR\->bit_fail_limit" 4
.IX Item "$ann->bit_fail_limit"
.PD 0
.ie n .IP "$ann\->bit_fail_limit($bfl)" 4
.el .IP "\f(CW$ann\fR\->bit_fail_limit($bfl)" 4
.IX Item "$ann->bit_fail_limit($bfl)"
.PD
\&\-
.ie n .IP "$ann\->quickprop_decay" 4
.el .IP "\f(CW$ann\fR\->quickprop_decay" 4
.IX Item "$ann->quickprop_decay"
.PD 0
.ie n .IP "$ann\->quickprop_decay($qpd)" 4
.el .IP "\f(CW$ann\fR\->quickprop_decay($qpd)" 4
.IX Item "$ann->quickprop_decay($qpd)"
.PD
\&\-
.ie n .IP "$ann\->quickprop_mu" 4
.el .IP "\f(CW$ann\fR\->quickprop_mu" 4
.IX Item "$ann->quickprop_mu"
.PD 0
.ie n .IP "$ann\->quickprop_mu($qpmu)" 4
.el .IP "\f(CW$ann\fR\->quickprop_mu($qpmu)" 4
.IX Item "$ann->quickprop_mu($qpmu)"
.PD
\&\-
.ie n .IP "$ann\->rprop_increase_factor" 4
.el .IP "\f(CW$ann\fR\->rprop_increase_factor" 4
.IX Item "$ann->rprop_increase_factor"
.PD 0
.ie n .IP "$ann\->rprop_increase_factor($factor)" 4
.el .IP "\f(CW$ann\fR\->rprop_increase_factor($factor)" 4
.IX Item "$ann->rprop_increase_factor($factor)"
.PD
\&\-
.ie n .IP "$ann\->rprop_decrease_factor" 4
.el .IP "\f(CW$ann\fR\->rprop_decrease_factor" 4
.IX Item "$ann->rprop_decrease_factor"
.PD 0
.ie n .IP "$ann\->rprop_decrease_factor($factor)" 4
.el .IP "\f(CW$ann\fR\->rprop_decrease_factor($factor)" 4
.IX Item "$ann->rprop_decrease_factor($factor)"
.PD
\&\-
.ie n .IP "$ann\->rprop_delta_min" 4
.el .IP "\f(CW$ann\fR\->rprop_delta_min" 4
.IX Item "$ann->rprop_delta_min"
.PD 0
.ie n .IP "$ann\->rprop_delta_min($min)" 4
.el .IP "\f(CW$ann\fR\->rprop_delta_min($min)" 4
.IX Item "$ann->rprop_delta_min($min)"
.PD
\&\-
.ie n .IP "$ann\->rprop_delta_max" 4
.el .IP "\f(CW$ann\fR\->rprop_delta_max" 4
.IX Item "$ann->rprop_delta_max"
.PD 0
.ie n .IP "$ann\->rprop_delta_max($max)" 4
.el .IP "\f(CW$ann\fR\->rprop_delta_max($max)" 4
.IX Item "$ann->rprop_delta_max($max)"
.PD
\&\-
.ie n .IP "$ann\->num_inputs" 4
.el .IP "\f(CW$ann\fR\->num_inputs" 4
.IX Item "$ann->num_inputs"
\&\-
.ie n .IP "$ann\->num_outputs" 4
.el .IP "\f(CW$ann\fR\->num_outputs" 4
.IX Item "$ann->num_outputs"
\&\-
.ie n .IP "$ann\->total_neurons" 4
.el .IP "\f(CW$ann\fR\->total_neurons" 4
.IX Item "$ann->total_neurons"
\&\-
.ie n .IP "$ann\->total_connections" 4
.el .IP "\f(CW$ann\fR\->total_connections" 4
.IX Item "$ann->total_connections"
\&\-
.ie n .IP "$ann\->\s-1MSE\s0" 4
.el .IP "\f(CW$ann\fR\->\s-1MSE\s0" 4
.IX Item "$ann->MSE"
\&\-
.ie n .IP "$ann\->bit_fail" 4
.el .IP "\f(CW$ann\fR\->bit_fail" 4
.IX Item "$ann->bit_fail"
\&\-
.IP "cascade_output_change_fraction" 4
.IX Item "cascade_output_change_fraction"
.PD 0
.IP "cascade_output_change_fraction($fraction)" 4
.IX Item "cascade_output_change_fraction($fraction)"
.PD
\&\-
.ie n .IP "$ann\->cascade_output_stagnation_epochs" 4
.el .IP "\f(CW$ann\fR\->cascade_output_stagnation_epochs" 4
.IX Item "$ann->cascade_output_stagnation_epochs"
.PD 0
.ie n .IP "$ann\->cascade_output_stagnation_epochs($epochs)" 4
.el .IP "\f(CW$ann\fR\->cascade_output_stagnation_epochs($epochs)" 4
.IX Item "$ann->cascade_output_stagnation_epochs($epochs)"
.PD
\&\-
.ie n .IP "$ann\->cascade_candidate_change_fraction" 4
.el .IP "\f(CW$ann\fR\->cascade_candidate_change_fraction" 4
.IX Item "$ann->cascade_candidate_change_fraction"
.PD 0
.ie n .IP "$ann\->cascade_candidate_change_fraction($fraction)" 4
.el .IP "\f(CW$ann\fR\->cascade_candidate_change_fraction($fraction)" 4
.IX Item "$ann->cascade_candidate_change_fraction($fraction)"
.PD
\&\-
.ie n .IP "$ann\->cascade_candidate_stagnation_epochs" 4
.el .IP "\f(CW$ann\fR\->cascade_candidate_stagnation_epochs" 4
.IX Item "$ann->cascade_candidate_stagnation_epochs"
.PD 0
.ie n .IP "$ann\->cascade_candidate_stagnation_epochs($epochs)" 4
.el .IP "\f(CW$ann\fR\->cascade_candidate_stagnation_epochs($epochs)" 4
.IX Item "$ann->cascade_candidate_stagnation_epochs($epochs)"
.PD
\&\-
.ie n .IP "$ann\->cascade_weight_multiplier" 4
.el .IP "\f(CW$ann\fR\->cascade_weight_multiplier" 4
.IX Item "$ann->cascade_weight_multiplier"
.PD 0
.ie n .IP "$ann\->cascade_weight_multiplier($multiplier)" 4
.el .IP "\f(CW$ann\fR\->cascade_weight_multiplier($multiplier)" 4
.IX Item "$ann->cascade_weight_multiplier($multiplier)"
.PD
\&\-
.ie n .IP "$ann\->cascade_candidate_limit" 4
.el .IP "\f(CW$ann\fR\->cascade_candidate_limit" 4
.IX Item "$ann->cascade_candidate_limit"
.PD 0
.ie n .IP "$ann\->cascade_candidate_limit($limit)" 4
.el .IP "\f(CW$ann\fR\->cascade_candidate_limit($limit)" 4
.IX Item "$ann->cascade_candidate_limit($limit)"
.PD
\&\-
.ie n .IP "$ann\->cascade_max_out_epochs" 4
.el .IP "\f(CW$ann\fR\->cascade_max_out_epochs" 4
.IX Item "$ann->cascade_max_out_epochs"
.PD 0
.ie n .IP "$ann\->cascade_max_out_epochs($epochs)" 4
.el .IP "\f(CW$ann\fR\->cascade_max_out_epochs($epochs)" 4
.IX Item "$ann->cascade_max_out_epochs($epochs)"
.PD
\&\-
.ie n .IP "$ann\->cascade_max_cand_epochs" 4
.el .IP "\f(CW$ann\fR\->cascade_max_cand_epochs" 4
.IX Item "$ann->cascade_max_cand_epochs"
.PD 0
.ie n .IP "$ann\->cascade_max_cand_epochs($epochs)" 4
.el .IP "\f(CW$ann\fR\->cascade_max_cand_epochs($epochs)" 4
.IX Item "$ann->cascade_max_cand_epochs($epochs)"
.PD
\&\-
.ie n .IP "$ann\->cascade_num_candidates" 4
.el .IP "\f(CW$ann\fR\->cascade_num_candidates" 4
.IX Item "$ann->cascade_num_candidates"
\&\-
.ie n .IP "$ann\->cascade_num_candidate_groups" 4
.el .IP "\f(CW$ann\fR\->cascade_num_candidate_groups" 4
.IX Item "$ann->cascade_num_candidate_groups"
.PD 0
.ie n .IP "$ann\->cascade_num_candidate_groups($groups)" 4
.el .IP "\f(CW$ann\fR\->cascade_num_candidate_groups($groups)" 4
.IX Item "$ann->cascade_num_candidate_groups($groups)"
.PD
\&\-
.ie n .IP "$ann\->neuron_activation_function($layer_index, $neuron_index)" 4
.el .IP "\f(CW$ann\fR\->neuron_activation_function($layer_index, \f(CW$neuron_index\fR)" 4
.IX Item "$ann->neuron_activation_function($layer_index, $neuron_index)"
.PD 0
.ie n .IP "$ann\->neuron_activation_function($layer_index, $neuron_index, $activation_function)" 4
.el .IP "\f(CW$ann\fR\->neuron_activation_function($layer_index, \f(CW$neuron_index\fR, \f(CW$activation_function\fR)" 4
.IX Item "$ann->neuron_activation_function($layer_index, $neuron_index, $activation_function)"
.PD
\&\-
.ie n .IP "$ann\->layer_activation_function($layer_index, $activation_function)" 4
.el .IP "\f(CW$ann\fR\->layer_activation_function($layer_index, \f(CW$activation_function\fR)" 4
.IX Item "$ann->layer_activation_function($layer_index, $activation_function)"
\&\-
.ie n .IP "$ann\->hidden_activation_function($layer_index, $activation_function)" 4
.el .IP "\f(CW$ann\fR\->hidden_activation_function($layer_index, \f(CW$activation_function\fR)" 4
.IX Item "$ann->hidden_activation_function($layer_index, $activation_function)"
\&\-
.ie n .IP "$ann\->output_activation_function($layer_index, $activation_function)" 4
.el .IP "\f(CW$ann\fR\->output_activation_function($layer_index, \f(CW$activation_function\fR)" 4
.IX Item "$ann->output_activation_function($layer_index, $activation_function)"
\&\-
.ie n .IP "$ann\->neuron_activation_steepness($layer_index, $neuron_index)" 4
.el .IP "\f(CW$ann\fR\->neuron_activation_steepness($layer_index, \f(CW$neuron_index\fR)" 4
.IX Item "$ann->neuron_activation_steepness($layer_index, $neuron_index)"
.PD 0
.ie n .IP "$ann\->neuron_activation_steepness($layer_index, $neuron_index, $activation_steepness)" 4
.el .IP "\f(CW$ann\fR\->neuron_activation_steepness($layer_index, \f(CW$neuron_index\fR, \f(CW$activation_steepness\fR)" 4
.IX Item "$ann->neuron_activation_steepness($layer_index, $neuron_index, $activation_steepness)"
.PD
\&\-
.ie n .IP "$ann\->layer_activation_steepness($layer_index, $activation_steepness)" 4
.el .IP "\f(CW$ann\fR\->layer_activation_steepness($layer_index, \f(CW$activation_steepness\fR)" 4
.IX Item "$ann->layer_activation_steepness($layer_index, $activation_steepness)"
\&\-
.ie n .IP "$ann\->hidden_activation_steepness($layer_index, $activation_steepness)" 4
.el .IP "\f(CW$ann\fR\->hidden_activation_steepness($layer_index, \f(CW$activation_steepness\fR)" 4
.IX Item "$ann->hidden_activation_steepness($layer_index, $activation_steepness)"
\&\-
.ie n .IP "$ann\->output_activation_steepness($layer_index, $activation_steepness)" 4
.el .IP "\f(CW$ann\fR\->output_activation_steepness($layer_index, \f(CW$activation_steepness\fR)" 4
.IX Item "$ann->output_activation_steepness($layer_index, $activation_steepness)"
\&\-
.ie n .IP "$ann\->num_layers" 4
.el .IP "\f(CW$ann\fR\->num_layers" 4
.IX Item "$ann->num_layers"
returns the number of layers on the \s-1ANN\s0
.ie n .IP "$ann\->layer_num_neurons($layer_index)" 4
.el .IP "\f(CW$ann\fR\->layer_num_neurons($layer_index)" 4
.IX Item "$ann->layer_num_neurons($layer_index)"
return the number of neurons on layer \f(CW$layer_index\fR.
.ie n .IP "$ann\->num_neurons" 4
.el .IP "\f(CW$ann\fR\->num_neurons" 4
.IX Item "$ann->num_neurons"
return a list with the number of neurons on every layer
.SS "AI::FANN::TrainData"
.IX Subsection "AI::FANN::TrainData"
Wraps C \f(CW\*(C`struct fann_train_data\*(C'\fR and provides the following method:
.IP "AI::FANN::TrainData\->new_from_file($filename)" 4
.IX Item "AI::FANN::TrainData->new_from_file($filename)"
\&\-
.ie n .IP "AI::FANN::TrainData\->new($input1, $output1 [, $input2, $output2, ...])" 4
.el .IP "AI::FANN::TrainData\->new($input1, \f(CW$output1\fR [, \f(CW$input2\fR, \f(CW$output2\fR, ...])" 4
.IX Item "AI::FANN::TrainData->new($input1, $output1 [, $input2, $output2, ...])"
\&\f(CW$inputx\fR and \f(CW$outputx\fR are arrays with the values of the input and
output layers.
.ie n .IP "AI::FANN::TrainData\->new_empty($num_data, $num_inputs, $num_outputs)" 4
.el .IP "AI::FANN::TrainData\->new_empty($num_data, \f(CW$num_inputs\fR, \f(CW$num_outputs\fR)" 4
.IX Item "AI::FANN::TrainData->new_empty($num_data, $num_inputs, $num_outputs)"
returns a new AI::FANN::TrainData object of the sizes indicated on the
arguments. The initial values of the data contained inside the object
are random and should be set before using the train data object for
training an \s-1ANN\s0.
.ie n .IP "$train\->data($index)" 4
.el .IP "\f(CW$train\fR\->data($index)" 4
.IX Item "$train->data($index)"
returns two arrays with the values of the input and output layer
respectively for that index.
.ie n .IP "$train\->data($index, $input, $output)" 4
.el .IP "\f(CW$train\fR\->data($index, \f(CW$input\fR, \f(CW$output\fR)" 4
.IX Item "$train->data($index, $input, $output)"
\&\f(CW$input\fR and \f(CW$output\fR are two arrays.
.Sp
The input and output layers at the index \f(CW$index\fR are set to the
values on these arrays.
.ie n .IP "$train\->shuffle" 4
.el .IP "\f(CW$train\fR\->shuffle" 4
.IX Item "$train->shuffle"
\&\-
.ie n .IP "$train\->scale_input($new_min, $new_max)" 4
.el .IP "\f(CW$train\fR\->scale_input($new_min, \f(CW$new_max\fR)" 4
.IX Item "$train->scale_input($new_min, $new_max)"
\&\-
.ie n .IP "$train\->scale_output($new_min, $new_max)" 4
.el .IP "\f(CW$train\fR\->scale_output($new_min, \f(CW$new_max\fR)" 4
.IX Item "$train->scale_output($new_min, $new_max)"
\&\-
.ie n .IP "$train\->scale($new_min, $new_max)" 4
.el .IP "\f(CW$train\fR\->scale($new_min, \f(CW$new_max\fR)" 4
.IX Item "$train->scale($new_min, $new_max)"
\&\-
.ie n .IP "$train\->subset($pos, $length)" 4
.el .IP "\f(CW$train\fR\->subset($pos, \f(CW$length\fR)" 4
.IX Item "$train->subset($pos, $length)"
\&\-
.ie n .IP "$train\->num_inputs" 4
.el .IP "\f(CW$train\fR\->num_inputs" 4
.IX Item "$train->num_inputs"
\&\-
.ie n .IP "$train\->num_outputs" 4
.el .IP "\f(CW$train\fR\->num_outputs" 4
.IX Item "$train->num_outputs"
\&\-
.ie n .IP "$train\->length" 4
.el .IP "\f(CW$train\fR\->length" 4
.IX Item "$train->length"
\&\-
.SH "INSTALLATION"
.IX Header "INSTALLATION"
See the \s-1README\s0 file for instruction on installing this module.
.SH "BUGS"
.IX Header "BUGS"
Only tested on Linux.
.PP
I/O is not performed through PerlIO because the C library doesn't have
the required infrastructure to do that.
.PP
Send bug reports to my email address or use the \s-1CPAN\s0 \s-1RT\s0 system.
.SH "SEE ALSO"
.IX Header "SEE ALSO"
\&\s-1FANN\s0 homepage at <http://leenissen.dk/fann/index.php>.
.SH "COPYRIGHT AND LICENSE"
.IX Header "COPYRIGHT AND LICENSE"
Copyright (C) 2006\-2008 by Salvador Fandin\*~o
(sfandino@yahoo.com).
.PP
This Perl module is free software; you can redistribute it and/or
modify it under the same terms as Perl itself, either Perl version
5.8.8 or, at your option, any later version of Perl 5 you may have
available.
.PP
The Fast Artificial Neural Network Library (\s-1FANN\s0)
Copyright (C) 2003\-2006 Steffen Nissen (lukesky@diku.dk) and others.
.PP
Distributed under the \s-1GNU\s0 Lesser General Public License.
